Problem (unicode1): Understanding Unicode (1 point)
(a) What Unicode character does chr(0) return?

Deliverable: 

Answer:
Will return NULL that is not printable

(b) How does this character‚Äôs string representation (__repr__()) differ from its printed representa-
tion?

Deliverable: 

Answer:
print(repr(chr(0))) will print '\x00' 

(c) What happens when this character occurs in text? It may be helpful to play around with the
following in your Python interpreter and see if it matches your expectations:
>>> chr(0)
>>> print(chr(0))
>>> "this is a test" + chr(0) + "string"
>>> print("this is a test" + chr(0) + "string")
Deliverable: 

Answer:
Will not print anything from chr(0)



Problem (unicode2): Unicode Encodings (3 points)
(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than
UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various
input strings.
Deliverable: A one-to-two sentence response.

Answer:
UTF-8 most common encoding on the web. Compact enough for a model
AœÄüöÄ
UTF-8 
41 CF 80 F0 9F 9A 80
Total: 7 bytes.

UTF-16 (Balanced but Bloated)
00 41 03 C0 D8 3D DE 80
Total: 8 bytes.

UTF-32 (Fixed & Massive)
00 00 00 41 00 00 03 C0 00 01 F6 80
Total: 12 bytes.


(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into
a Unicode string. Why is this function incorrect? Provide an example of an input byte string
that yields incorrect results.
def decode_utf8_bytes_to_str_wrong(bytestring: bytes):
return "".join([bytes([b]).decode("utf-8") for b in bytestring])
>>> decode_utf8_bytes_to_str_wrong("hello".encode("utf-8"))
'hello'
Deliverable: An example input byte string for which decode_utf8_bytes_to_str_wrong pro-
duces incorrect output, with a one-sentence explanation of why the function is incorrect.

Answer:
# The byte string for "√©"
input_bytes = b'\xc3\xa9'

# This will raise a UnicodeDecodeError
decode_utf8_bytes_to_str_wrong(input_bytes)

The function is incorrect because it attempts to decode each byte in isolation, 
failing to recognize that UTF-8 is a variable-width encoding where a single 
character (like an emoji or accented letter) is often composed of a specific sequence of multiple bytes.


(c) Give a two byte sequence that does not decode to any Unicode character(s).
Deliverable: An example, with a one-sentence explanation

Answer:
0xC0 0xAF
0xC0 is an invalid start byte because it's "overlong."
0xAF is a valid continuation byte, but because the first byte is forbidden, 
the decoder will throw an error or display a replacement character (like ****).




Problem (train_bpe_tinystories): BPE Training on TinyStories (2 points)
(a) Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size
of 10,000. Make sure to add the TinyStories <|endoftext|> special token to the vocabulary.
Serialize the resulting vocabulary and merges to disk for further inspection. How many hours
and memory did training take? What is the longest token in the vocabulary? Does it make sense?
Resource requirements: ‚â§30 minutes (no GPUs), ‚â§ 30GB RAM
Hint You should be able to get under 2 minutes for BPE training using multiprocessing during
pretokenization and the following two facts:
(a) The <|endoftext|> token delimits documents in the data files.
(b) The <|endoftext|> token is handled as a special case before the BPE merges are applied.
Deliverable: A one-to-two sentence response.

Answer:
% uv run python -m ece405_basics.train_bpe_tinystories
    Learned vocab size: 10000
    Learned merges: 9743

    Vocabulary serialized to ../ece405_basics/bpe_output/vocab.json
    Merges serialized to ../ece405_basics/bpe_output/merges.json

What is the longest token in the vocabulary?

    Longest token length: 15 bytes
    Longest token:  accomplishment

How many hours and memory did training take?

    Training time: 0.0204 hours (73.47 seconds)
    Memory usage: 169.73 MB

Does it make sense?

    Looks fine.


(b) Profile your code. What part of the tokenizer training process takes the most time?
Deliverable: A one-to-two sentence response.

Answer:
% uv run python -m cProfile -o profile.stats -m ece405_basics.train_bpe_tinystories
% uv run python -c "import pstats; p = pstats.Stats('profile.stats'); p.sort_stats('cumulative').print_stats(30)"

Lambda function in max() - 36% of runtime




Problem (train_bpe_expts_owt): BPE Training on OpenWebText (2 points)
(a) Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary
size of 32,000. Serialize the resulting vocabulary and merges to disk for further inspection. What
is the longest token in the vocabulary? Does it make sense?
Resource requirements: ‚â§12 hours (no GPUs), ‚â§ 100GB RAM
Deliverable: A one-to-two sentence response.


% python3 ece405_basics/train_bpe_expts_owt.py  
Learned vocab size: 32000
Learned merges: 31743

Vocabulary serialized to /ece405_basics/bpe_output/vocab_owt.json
Merges serialized to /ece405_basics/bpe_output/merges_owt.json

Longest token length: 64 bytes
Longest token: √É√Ç√É√Ç√É√Ç√É√Ç√É√Ç√É√Ç√É√Ç√É√Ç√É√Ç√É√Ç√É√Ç√É√Ç√É√Ç√É√Ç√É√Ç√É√Ç

Training time: 5.0110 hours (18039.62 seconds)
Memory usage: 10873.41 MB



(b) Compare and contrast the tokenizer that you get training on TinyStories versus OpenWebText.
Deliverable: A one-to-two sentence response.

73.47 seconds vs. 5.0110 hours
vocab size: 10000 vs. 32000
Memory usage: 169.73 MB vs 10873.41 MB


2.7 Experiments
Problem (tokenizer_experiments): Experiments with tokenizers (4 points)
(a) Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyS-
tories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these
sampled documents into integer IDs. What is each tokenizer‚Äôs compression ratio (bytes/token)?
Deliverable: A one-to-two sentence response.

Answer:

python3 ece405_basics/tokenizer_experiments.py
TinyStories tokenizer (10k vocab) on TinyStories sample (10 docs):
  total bytes   = 7402
  total tokens  = 1730
  bytes/token   = 4.2786

OpenWebText tokenizer (32k vocab) on OpenWebText sample (10 docs):
  total bytes   = 34104
  total tokens  = 7552
  bytes/token   = 4.5159

(b) What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Com-
pare the compression ratio and/or qualitatively describe what happens.
Deliverable: A one-to-two sentence response.

Answer:

% python3 ece405_basics/tokenizer_experiments.py

Tokenizing the OpenWebText sample with the TinyStories tokenizer 
gives significantly worse compression than the OpenWebText tokenizer: 
about 3.31 bytes/token vs 4.52 bytes/token (so the TinyStories tokenizer needs ~36% more tokens for the same text).

(c) Estimate the throughput of your tokenizer (e.g., in bytes/second). How long would it take to
tokenize the Pile dataset (825GB of text)?
Deliverable: A one-to-two sentence response.

Answer:

% python3 ece405_basics/tokenizer_experiments.py

TinyStories: elapsed encode time 0.000919 s, 8.051 MB/s, 28.46 hours for 825GB
OpenWebText: elapsed encode time 0.003596 s, 9.485 MB/s, 24.16 hours for 825GB

(d) Using your TinyStor'
ies and OpenWebText tokenizers, encode the respectiv
e training and devel-
opment datasets into a sequence of in
teger token IDs. We‚Äôll use this later to train our language
model. We recommend se
rializing the token IDs as a NumPy array of datatype uint16. Why is
uint16 an app
ropriate choice?

Answer:

uint16 is appropriate because token IDs only need to cover the vocabulary size, 
and both of tokenizers are far below 65536 IDs (TinyStories: 10k, OpenWebText: 32k, 
plus special tokens). Using uint16 therefore stores every token ID safely while 
cutting storage roughly  in half versus int32 (and to one-quarter of int64), 
which is important for large encoded train/dev arrays.

